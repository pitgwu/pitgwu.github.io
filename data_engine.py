import yfinance as yf
import pandas as pd
import numpy as np
import requests
import json
import os
import datetime
import time
from io import StringIO  # æ–°å¢: ç”¨æ–¼æ¶ˆé™¤ Pandas è­¦å‘Š

# ==========================================
# 0. è¨­å®šèˆ‡ç›®éŒ„æº–å‚™
# ==========================================
NOW = datetime.datetime.now()
YYYY = NOW.strftime("%Y")
MM = NOW.strftime("%m")
DATE_STR = NOW.strftime("%Y%m%d")

BASE_DIR = "us_stock_dashboard"
TARGET_DIR = os.path.join(BASE_DIR, YYYY, MM)

os.makedirs(TARGET_DIR, exist_ok=True)

print(f"ğŸ“‚ ç›®æ¨™è³‡æ–™å¤¾: {TARGET_DIR}")
print(f"ğŸ“… è™•ç†æ—¥æœŸ: {DATE_STR}")

# ==========================================
# 1. éœæ…‹è§€å¯Ÿåå–® (æ ¸å¿ƒæ¬Šå€¼è‚¡)
# ==========================================
STATIC_TICKERS = {
    # æŒ‡æ•¸
    "^DJI": {"Name": "Dow Jones", "Theme": "é“ç“Š"},
    "^GSPC": {"Name": "S&P 500", "Theme": "æ¨™æ™®"},
    "^IXIC": {"Name": "Nasdaq", "Theme": "é‚£æŒ‡"},
    "^SOX": {"Name": "PHLX Semi", "Theme": "è²»åŠ"},
    "^VIX": {"Name": "VIX", "Theme": "ææ…Œ"},
    "BTC-USD": {"Name": "Bitcoin", "Theme": "åŠ å¯†å¹£"},
    # å·¨é ­
    "NVDA": {"Name": "NVIDIA", "Theme": "AI"},
    "MSFT": {"Name": "Microsoft", "Theme": "è»Ÿé«”"},
    "AAPL": {"Name": "Apple", "Theme": "æ¶ˆè²»é›»"},
    "AMZN": {"Name": "Amazon", "Theme": "é›»å•†"},
    "GOOG": {"Name": "Alphabet", "Theme": "æœå°‹"},
    "META": {"Name": "Meta", "Theme": "ç¤¾ç¾¤"},
    "TSLA": {"Name": "Tesla", "Theme": "é›»å‹•è»Š"},
    "TSM": {"Name": "TSMC", "Theme": "æ™¶åœ“"},
    "AMD": {"Name": "AMD", "Theme": "æ™¶ç‰‡"},
    "AVGO": {"Name": "Broadcom", "Theme": "ç¶²é€š"},
    "SMCI": {"Name": "Super Micro", "Theme": "ä¼ºæœå™¨"},
    "COIN": {"Name": "Coinbase", "Theme": "å¹£æ‰€"},
}

ALL_TICKER_INFO = STATIC_TICKERS.copy()

# ==========================================
# 2. å‹•æ…‹å¸‚å ´æƒæ (Web Scraping)
# ==========================================
def get_market_screeners():
    print("ğŸ” æ­£åœ¨çˆ¬å– Yahoo ç¶²é ç†±é–€æ¦œ (Web Scraping)...")
    
    targets = [
        ("https://finance.yahoo.com/most-active", "ğŸ”¥ äº¤æ˜“ç†±é–€"),
        ("https://finance.yahoo.com/gainers", "ğŸš€ æ¼²å¹…æ’è¡Œ")
    ]
    
    found_tickers = []
    
    # å½è£ç€è¦½å™¨ Header
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    for url, tag in targets:
        try:
            print(f"   -> æ­£åœ¨è®€å–: {url} ...")
            r = requests.get(url, headers=headers, timeout=10)
            
            # ã€ä¿®æ­£é»ã€‘ä½¿ç”¨ StringIO åŒ…è£ HTML å­—ä¸²ï¼Œæ¶ˆé™¤ FutureWarning
            dfs = pd.read_html(StringIO(r.text))
            
            if len(dfs) > 0:
                df = dfs[0]
                symbols = df.iloc[:, 0].tolist()
                
                count = 0
                for sym in symbols:
                    sym = str(sym).split(" ")[0]
                    if "." not in sym and len(sym) < 6:
                        found_tickers.append(sym)
                        if sym not in ALL_TICKER_INFO:
                            try:
                                name = df.iloc[count, 1]
                            except:
                                name = sym
                            ALL_TICKER_INFO[sym] = {"Name": name, "Theme": tag}
                        count += 1
                print(f"      æŠ“åˆ° {count} æª”")
            else:
                print(f"      âš ï¸ æœªåœ¨é é¢ä¸­ç™¼ç¾è¡¨æ ¼")

        except Exception as e:
            print(f"      âš ï¸ çˆ¬å–å¤±æ•—: {e}")

    # ä¿åº•æ¸…å–®
    if not found_tickers:
        print("âš ï¸ çˆ¬èŸ²å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨ç†±é–€æ¸…å–®")
        backup_list = ["PLTR", "SOFI", "MARA", "RIOT", "DKNG", "UBER", "HOOD", "OPEN", "LCID", "RIVN", "AMD", "F", "BAC", "T", "INTC"]
        for sym in backup_list:
            if sym not in ALL_TICKER_INFO:
                ALL_TICKER_INFO[sym] = {"Name": sym, "Theme": "å‚™ç”¨ç†±é–€"}
        found_tickers = backup_list

    dynamic_list = list(set(found_tickers))
    print(f"âœ… æƒæå®Œæˆï¼å…±é–å®š {len(dynamic_list)} æª”æ´»èºè‚¡ç¥¨ã€‚")
    return dynamic_list

def fetch_fear_and_greed():
    print("æ­£åœ¨æŠ“å– CNN Fear & Greed æŒ‡æ•¸...")
    try:
        url = "https://production.dataviz.cnn.io/index/fearandgreed/graphdata"
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            latest = data['fear_and_greed']
            result = {"score": int(latest['score']), "rating": latest['rating'], "timestamp": latest['timestamp']}
            filepath = os.path.join(TARGET_DIR, f"sentiment_{DATE_STR}.json")
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=4)
            print(f"âœ… CNN æŒ‡æ•¸å·²å­˜æª”")
            return
    except: pass
    
    filepath = os.path.join(TARGET_DIR, f"sentiment_{DATE_STR}.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump({"score": 50, "rating": "Neutral", "timestamp": ""}, f)

def fetch_and_process_data():
    fetch_fear_and_greed()
    
    dynamic_tickers = get_market_screeners()
    static_list = list(STATIC_TICKERS.keys())
    final_tickers = list(set(static_list + dynamic_tickers))
    
    print(f"ğŸš€ æº–å‚™ä¸‹è¼‰ {len(final_tickers)} æª”æ¨™çš„æ•¸æ“š...")
    
    results = []
    BATCH_SIZE = 8
    chunks = [final_tickers[i:i + BATCH_SIZE] for i in range(0, len(final_tickers), BATCH_SIZE)]
    
    for i, chunk in enumerate(chunks):
        max_retries = 5
        success = False
        data = pd.DataFrame()

        for attempt in range(max_retries):
            try:
                print(f"â³ ä¸‹è¼‰æ‰¹æ¬¡ {i+1}/{len(chunks)} (å˜—è©¦ {attempt+1}/{max_retries}): {chunk[:3]}...")
                data = yf.download(chunk, period="3mo", progress=False, auto_adjust=False, threads=False)
                if not data.empty:
                    success = True
                    break
            except Exception as e:
                print(f"   âš ï¸ éŒ¯èª¤: {e}")
            time.sleep(5)

        if not success or data.empty:
            print(f"âŒ æ‰¹æ¬¡ {i+1} æœ€çµ‚å¤±æ•—ï¼Œè·³éã€‚")
            continue
        
        for ticker in chunk:
            try:
                if len(chunk) == 1 or isinstance(data.columns, pd.Index) and not isinstance(data.columns, pd.MultiIndex):
                    closes = data['Close']
                    volumes = data['Volume']
                else:
                    try:
                        closes = data['Close'][ticker].dropna()
                        volumes = data['Volume'][ticker].dropna()
                    except KeyError: continue

                if closes.empty: continue
                
                current_price = closes.iloc[-1]
                
                # ã€æ–°å¢ã€‘éæ¿¾æ‰è‚¡åƒ¹ < 1 ç¾å…ƒçš„æ°´é¤ƒè‚¡ (Penny Stocks)
                if current_price < 1.0:
                    continue

                current_vol = 0 if volumes.empty else volumes.iloc[-1]
                
                if len(volumes) >= 6:
                    avg_vol_5d = volumes.iloc[-6:-1].mean()
                    rvol = (current_vol / avg_vol_5d) if avg_vol_5d > 0 else 0
                else:
                    rvol = 0

                def calc_chg(series, shift):
                    if len(series) > shift:
                        last = series.iloc[-1]
                        prev = series.iloc[-(shift + 1)]
                        return ((last - prev) / prev) * 100 if prev != 0 else 0
                    return 0

                def calc_vol_chg(series, shift):
                    if len(series) > shift:
                        last = series.iloc[-1]
                        prev = series.iloc[-(shift + 1)]
                        return ((last - prev) / prev) * 100 if prev != 0 else 0
                    return 0

                daily_chg = calc_chg(closes, 1)
                weekly_chg = calc_chg(closes, 5)
                monthly_chg = calc_chg(closes, 21)
                
                daily_vol_chg = calc_vol_chg(volumes, 1)
                weekly_vol_chg = calc_vol_chg(volumes, 5)
                monthly_vol_chg = calc_vol_chg(volumes, 21)
                
                amount_b = (current_price * current_vol) / 1_000_000_000
                info = ALL_TICKER_INFO.get(ticker, {"Name": ticker, "Theme": "Scan"})

                results.append({
                    "Code": ticker,
                    "Name": info.get('Name', ticker),
                    "Theme": info.get('Theme', 'Scan'),
                    "Close": round(current_price, 2),
                    "Volume": current_vol,
                    "RVOL": round(rvol, 2),
                    "Daily_Chg%": round(daily_chg, 2),
                    "Weekly_Chg%": round(weekly_chg, 2),
                    "Monthly_Chg%": round(monthly_chg, 2),
                    "Daily_Vol_Chg%": round(daily_vol_chg, 2),
                    "Weekly_Vol_Chg%": round(weekly_vol_chg, 2),
                    "Monthly_Vol_Chg%": round(monthly_vol_chg, 2),
                    "Daily_Amount_B": round(amount_b, 2),
                    "Weekly_Amount_B": round(amount_b, 2),
                    "Monthly_Amount_B": round(amount_b, 2)
                })

            except Exception: continue
        
        time.sleep(1)

    if not results:
        print("âŒ åš´é‡è­¦å‘Šï¼šç„¡æ•¸æ“šï¼")
    else:
        df_result = pd.DataFrame(results)
        for fname in [f"rank_daily_{DATE_STR}.csv", f"rank_weekly_{DATE_STR}.csv", f"rank_monthly_{DATE_STR}.csv"]:
            df_result.to_csv(os.path.join(TARGET_DIR, fname), index=False, encoding='utf-8-sig')
            
        print(f"\nâœ… æ•¸æ“šæ›´æ–°å®Œæˆï¼å…±è™•ç† {len(df_result)} æª”è‚¡ç¥¨ã€‚")

if __name__ == "__main__":
    fetch_and_process_data()
