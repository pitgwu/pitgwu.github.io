import pandas as pd
import numpy as np
import requests
import json
import os
import datetime
import time
import random
import re
from io import StringIO
import yfinance as yf
import urllib3

# é—œé–‰ SSL è­¦å‘Š (é‡å°æœŸäº¤æ‰€)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# 0. å…¨åŸŸè¨­å®š
# ==========================================
NOW = datetime.datetime.now()
YYYY = NOW.strftime("%Y")
MM = NOW.strftime("%m")
DATE_STR = NOW.strftime("%Y%m%d")

BASE_DIR = "tw_stock_dashboard"
TARGET_DIR = os.path.join(BASE_DIR, YYYY, MM)
os.makedirs(TARGET_DIR, exist_ok=True)

print(f"ğŸ“‚ ç›®æ¨™è³‡æ–™å¤¾: {TARGET_DIR}")
print(f"ğŸ“… è™•ç†æ—¥æœŸ: {DATE_STR}")

# ==========================================
# 1. åå–®èˆ‡è¨­å®š
# ==========================================
INDICES_CODES = {
    "^TWII": "åŠ æ¬ŠæŒ‡æ•¸", "^TWOII": "æ«ƒè²·æŒ‡æ•¸", 
    "^DJI": "é“ç“Šå·¥æ¥­", "^GSPC": "S&P 500", "^IXIC": "é‚£æ–¯é”å…‹", "^SOX": "è²»åŸåŠå°é«”", "^VIX": "ç¾è‚¡ VIX",
    "GC=F": "é»ƒé‡‘", "SI=F": "ç™½éŠ€", "HG=F": "éŠ…", "CL=F": "åŸæ²¹"
}

HIGH_PRICE_CODES = [
    "5274.TWO", "6669.TW", "3661.TW", "7769.TWO", "6515.TW", "2059.TW", "3008.TW", "3443.TW", "3653.TW", "6510.TWO",
    "6223.TWO", "3131.TWO", "3529.TWO", "2330.TW", "8299.TWO", "2383.TW", "2454.TW", "3665.TW", "6805.TW", "3017.TW",
    "3533.TW", "5269.TW", "6442.TW", "6781.TW", "2345.TW", "2308.TW", "6409.TW", "2404.TW", "7734.TWO", "1590.TW",
    "3324.TW", "8210.TW", "4749.TWO", "2360.TW", "7750.TWO", "5536.TWO", "3491.TWO", "1519.TW", "6944.TWO", "6739.TWO",
    "7751.TWO", "3293.TWO", "7805.TWO", "6640.TWO", "5289.TWO", "4583.TW", "2368.TW", "3081.TWO", "4966.TWO", "7728.TWO"
]

# æ“´å……ç”¢æ¥­å°ç…§ (ç•¶ Yahoo API å¤±æ•ˆæ™‚ä½¿ç”¨)
HP_SECTOR_MAP = {
    "2330.TW": "åŠå°é«”", "2317.TW": "é›»å­ä»£å·¥", "2454.TW": "ICè¨­è¨ˆ", "3008.TW": "å…‰å­¸é¡é ­", 
    "5274.TWO": "ä¼ºæœå™¨ç®¡ç†IC", "3661.TW": "ASIC", "6669.TW": "AIä¼ºæœå™¨", "2382.TW": "AIä¼ºæœå™¨",
    "2059.TW": "æ»‘è»Œ", "3443.TW": "ICè¨­è¨ˆ", "3653.TW": "æ•£ç†±", "6510.TWO": "æ¸¬è©¦ä»‹é¢",
    "6223.TWO": "æ¢é‡å¡", "3131.TWO": "åŠå°é«”è¨­å‚™", "3529.TWO": "çŸ½æ™ºè²¡", "8299.TWO": "NANDæ§åˆ¶",
    "2383.TW": "éŠ…ç®”åŸºæ¿", "3665.TW": "é€£æ¥å™¨", "6805.TW": "è»¸æ‰¿", "3017.TW": "æ•£ç†±",
    "3533.TW": "é€£æ¥å™¨", "5269.TW": "ICè¨­è¨ˆ", "6442.TW": "å…‰é€šè¨Š", "6781.TW": "é›»æ± æ¨¡çµ„",
    "2345.TW": "ç¶²é€š", "2308.TW": "é›»æºä¾›æ‡‰", "6409.TW": "ä¸æ–·é›»ç³»çµ±", "2404.TW": "ç„¡å¡µå®¤å·¥ç¨‹",
    "3324.TW": "æ•£ç†±", "8210.TW": "æ©Ÿæ®¼", "2360.TW": "æª¢æ¸¬è¨­å‚™", "1519.TW": "é‡é›»",
    "3293.TWO": "éŠæˆ²", "2368.TW": "PCB", "3081.TWO": "å…‰é€šè¨Š", "4966.TWO": "ICè¨­è¨ˆ"
}

# ==========================================
# 2. çˆ¬èŸ²åŠŸèƒ½
# ==========================================
def fetch_fear_and_greed():
    print("æ­£åœ¨æŠ“å– CNN Fear & Greed æŒ‡æ•¸...")
    try:
        r = requests.get("https://production.dataviz.cnn.io/index/fearandgreed/graphdata", headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        if r.status_code == 200:
            data = r.json()['fear_and_greed']
            with open(os.path.join(TARGET_DIR, f"sentiment_{DATE_STR}.json"), "w") as f:
                json.dump({"score": int(data['score']), "rating": data['rating']}, f)
    except: pass

def get_tw_vix_from_taifex():
    print("ğŸ” æ­£åœ¨å¾æœŸäº¤æ‰€æŠ“å–å°æŒ‡ VIX (å¼·åŒ–ç‰ˆ)...")
    url = "https://www.taifex.com.tw/cht/7/vixMinNew"
    # ä½¿ç”¨æ›´åƒçœŸäººçš„ Header ä¸¦ä¸”æ¥å—ä¸­æ–‡ç·¨ç¢¼
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
    }
    
    for i in range(3):
        try:
            # verify=False å¿½ç•¥ SSL æ†‘è­‰å•é¡Œ
            r = requests.get(url, headers=headers, timeout=15, verify=False)
            r.encoding = 'utf-8'
            dfs = pd.read_html(StringIO(r.text))
            if dfs:
                df = dfs[0]
                # å€’åºæª¢æŸ¥æœ€å¾Œä¸€ç­†æœ‰æ•ˆæ•¸å€¼
                for idx in range(len(df)-1, -1, -1):
                    try:
                        # å˜—è©¦å–å¾—æœ€å¾Œä¸€å€‹æ¬„ä½ (é€šå¸¸æ˜¯æŒ‡æ•¸)
                        val = df.iloc[idx].iat[-1]
                        v = float(val)
                        if 5 < v < 100: 
                            print(f"   âœ… å°æŒ‡ VIX: {v}")
                            return v
                    except: continue
                break
        except Exception as e:
            # print(f"VIX retry {i}: {e}")
            time.sleep(1)
    
    print("   âš ï¸ VIX æŠ“å–å¤±æ•—")
    return None

# --- Plan A: Yahoo API ---
def call_yahoo_api(url):
    # éš¨æ©Ÿ User-Agent
    uas = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    ]
    headers = { "User-Agent": random.choice(uas), "Referer": "https://tw.stock.yahoo.com/" }
    try:
        time.sleep(random.uniform(0.8, 2.0)) # å¢åŠ å»¶é²ï¼Œé™ä½è¢«é–æ©Ÿç‡
        r = requests.get(url, headers=headers, timeout=10)
        return r.json() if r.status_code == 200 else None
    except: return None

def fetch_yahoo_rankings(rank_type, limit=250):
    print(f"   [Yahoo] æŠ“å– {rank_type} ...")
    results = []
    for ex in ['TAI', 'TWO']:
        url = f"https://tw.stock.yahoo.com/_td-stock/api/resource/StockServices.rank;exchange={ex};rankCategory={rank_type};limit={limit}"
        data = call_yahoo_api(url)
        if data and 'list' in data:
            for item in data['list']:
                p = float(item.get('price', 0) or 0)
                vol_k = float(item.get('volumeK', 0) or 0) * 1000
                amt = float(item.get('turnoverM', 0) or 0) / 100
                if amt == 0 and p > 0: amt = (vol_k * p) / 100_000_000
                
                results.append({
                    "Code": item.get('symbol', ''), "Name": item.get('name', ''), "Close": p,
                    "Daily_Chg%": float(item.get('changePercent', 0) or 0),
                    "Daily_Amount_B": amt, "Volume": vol_k,
                    "Sector": item.get('sectorName', 'å…¶ä»–')
                })
    return pd.DataFrame(results)

def fetch_yahoo_quotes(symbols):
    if not symbols: return pd.DataFrame()
    results = []
    print(f"   [Yahoo] è£œæŠ“ {len(symbols)} æª”é—œæ³¨è‚¡...")
    
    batch_size = 20
    for i in range(0, len(symbols), batch_size):
        batch = symbols[i:i+batch_size]
        url = f"https://tw.stock.yahoo.com/_td-stock/api/resource/StockServices.quote;symbols={','.join(batch)}"
        data = call_yahoo_api(url)
        if data and 'list' in data:
            for item in data['list']:
                sym = item.get('symbol', '')
                p = float(item.get('price', 0) or 0)
                sector = "å¸‚å ´æŒ‡æ•¸" if "^" in sym else item.get('sectorName', 'å…¶ä»–')
                name = INDICES_CODES.get(sym, item.get('name', sym))
                
                vol = float(item.get('volume', 0) or 0) * 1000
                amt = float(item.get('turnoverM', 0) or 0) / 100
                if amt == 0 and p > 0: amt = (vol * p) / 100_000_000

                results.append({
                    "Code": sym, "Name": name, "Close": p, 
                    "Daily_Chg%": float(item.get('changePercent', 0) or 0),
                    "Daily_Amount_B": amt, "Volume": vol, "Sector": sector
                })
    return pd.DataFrame(results)

# --- Plan B: å®˜æ–¹å ±è¡¨ (TWSE/TPEX/Emerging) ---
def clean_number(x):
    if isinstance(x, str):
        x = re.sub(r'<[^>]+>|,|X|\+', '', x).strip()
        if x in ['--', '---', '']: return 0.0
        try: return float(x)
        except: return 0.0
    return x

def fetch_official_market(date_dt):
    print(f"   ğŸš‘ [æ•‘æ´æ¨¡å¼] ä¸‹è¼‰ {date_dt.strftime('%Y-%m-%d')} å®˜æ–¹å ±è¡¨...")
    date_str = date_dt.strftime("%Y%m%d")
    roc_year = date_dt.year - 1911
    date_roc = f"{roc_year}/{date_dt.month:02d}/{date_dt.day:02d}"
    
    dfs = []
    # ä¸Šå¸‚
    try:
        r = requests.get(f"https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX?date={date_str}&type=ALLBUT0999&response=json", timeout=10)
        data = r.json()
        if data['stat'] == 'OK':
            tbl = next((t['data'] for t in data.get('tables', []) if 'æ”¶ç›¤åƒ¹' in t['fields']), [])
            if tbl:
                d = pd.DataFrame(tbl).iloc[:, [0, 1, 2, 8, 9, 10]]
                d.columns = ['Code', 'Name', 'Volume', 'Close', 'Sign', 'ChgVal']
                d['Code'] = d['Code'].astype(str) + ".TW"
                dfs.append(d)
    except: pass
    # ä¸Šæ«ƒ
    try:
        r = requests.get(f"https://www.tpex.org.tw/web/stock/aftertrading/daily_close_quotes/stk_quote_result.php?l=zh-tw&d={date_roc}&s=0,asc,0&o=json", timeout=10)
        data = r.json()
        if 'aaData' in data:
            d = pd.DataFrame(data['aaData']).iloc[:, [0, 1, 8, 2, 3]]
            d.columns = ['Code', 'Name', 'Volume', 'Close', 'ChgVal']
            d['Code'] = d['Code'].astype(str) + ".TWO"
            d['Sign'] = ''
            dfs.append(d)
    except: pass
    # èˆˆæ«ƒ
    try:
        r = requests.get(f"https://www.tpex.org.tw/web/emergingstock/historical/daily/EMDaily_result.php?l=zh-tw&d={date_roc}&s=0,asc,0&o=json", timeout=10)
        data = r.json()
        if 'aaData' in data:
            d = pd.DataFrame(data['aaData']).iloc[:, [0, 1, 9, 6, 7]]
            d.columns = ['Code', 'Name', 'Volume', 'Close', 'ChgVal']
            d['Code'] = d['Code'].astype(str) + ".TWO"
            d['Sign'] = ''
            dfs.append(d)
    except: pass

    if not dfs: return None
    
    df_all = pd.concat(dfs, ignore_index=True)
    
    for c in ['Volume', 'Close', 'ChgVal']: df_all[c] = df_all[c].apply(clean_number)
    
    def parse_chg(row):
        v = row['ChgVal']
        s = str(row.get('Sign', ''))
        return -abs(v) if '-' in s or 'green' in s else abs(v)
    
    df_all['ChgAmt'] = df_all.apply(parse_chg, axis=1)
    df_all['Prev'] = df_all['Close'] - df_all['ChgAmt']
    
    def calc_pct(row):
        if row['Prev'] > 0: return (row['ChgAmt'] / row['Prev']) * 100
        return 0.0

    df_all['Daily_Chg%'] = df_all.apply(calc_pct, axis=1)
    df_all['Daily_Amount_B'] = (df_all['Volume'] * df_all['Close']) / 100_000_000
    
    # è£œä¸Šç”¢æ¥­ (ä½¿ç”¨éœæ…‹ Map)
    df_all['Sector'] = 'ä¸€èˆ¬'
    for code, sector in HP_SECTOR_MAP.items():
        df_all.loc[df_all['Code'] == code, 'Sector'] = sector
    
    # æ¨™è¨˜é«˜åƒ¹è‚¡ (å¦‚æœä¸åœ¨ Map è£¡ä½†ä¹Ÿè¨±æ˜¯èˆˆæ«ƒé«˜åƒ¹)
    df_all.loc[df_all['Code'].isin(HIGH_PRICE_CODES) & (df_all['Sector']=='ä¸€èˆ¬'), 'Sector'] = 'é«˜åƒ¹è‚¡'
    
    return df_all[['Code', 'Name', 'Close', 'Daily_Chg%', 'Daily_Amount_B', 'Volume', 'Sector']]

def generate_ai_analysis_placeholder():
    print("\nğŸš§ [Mark] AI åˆ†ææš«åœ...")
    with open(os.path.join(TARGET_DIR, f"ai_report_{DATE_STR}.json"), "w", encoding="utf-8") as f: json.dump([], f)

# ==========================================
# 4. ä¸»æµç¨‹
# ==========================================
def fetch_and_process_data():
    fetch_fear_and_greed()
    tw_vix = get_tw_vix_from_taifex()
    
    print("ğŸš€ å•Ÿå‹• V15.0 æ··åˆæ•¸æ“šå¼•æ“...")
    
    # 1. å˜—è©¦ Yahoo API (Aè¨ˆç•«)
    df_yahoo = pd.DataFrame()
    try:
        t200 = fetch_yahoo_rankings('turnover', 250)
        c200 = fetch_yahoo_rankings('changeUp', 250)
        targets = list(set(HIGH_PRICE_CODES + list(INDICES_CODES.keys())))
        quotes = fetch_yahoo_quotes(targets)
        df_yahoo = pd.concat([t200, c200, quotes], ignore_index=True)
    except: pass

    df_final = df_yahoo
    
    # 2. è‹¥ Yahoo å¤±æ•—ï¼Œå•Ÿå‹•å®˜æ–¹æ•‘æ´ (Bè¨ˆç•«)
    if df_final.empty or len(df_final) < 50:
        print("âŒ Yahoo API è³‡æ–™ä¸è¶³ï¼Œåˆ‡æ›è‡³å®˜æ–¹å ±è¡¨æ•‘æ´...")
        
        d = NOW
        if NOW.hour < 14: d -= datetime.timedelta(days=1)
        
        df_official = None
        for _ in range(5):
            df_official = fetch_official_market(d)
            if df_official is not None and not df_official.empty:
                print(f"   âœ… å®˜æ–¹å ±è¡¨ç²å–æˆåŠŸ: {d.strftime('%Y-%m-%d')}")
                break
            d -= datetime.timedelta(days=1)
            
        if df_official is not None:
            # âœ¨ [é—œéµä¿®æ­£] ä½¿ç”¨ yfinance è£œæŠ“åœ‹éš›æŒ‡æ•¸ (ç¹é Yahoo API é™åˆ¶)
            print("   ğŸŒ é€é yfinance è£œå……åœ‹éš›æŒ‡æ•¸ (ç¹é“æ¨¡å¼)...")
            try:
                # yfinance å¥—ä»¶ä½¿ç”¨ä¸åŒçš„å­˜å–æ©Ÿåˆ¶ï¼Œé€šå¸¸è¼ƒè€å°é–
                yf_tickers = list(INDICES_CODES.keys())
                # æ’é™¤ VIXTWN, å®ƒæ˜¯å°è‚¡
                yf_tickers = [t for t in yf_tickers if t != "^VIXTWN"]
                
                yf_data = yf.download(yf_tickers, period="5d", progress=False)
                
                idx_rows = []
                for t in yf_tickers:
                    try:
                        # è™•ç† MultiIndex æˆ– SingleIndex
                        if len(yf_tickers) > 1: c = yf_data['Close'][t]
                        else: c = yf_data['Close']
                        
                        c = c.dropna()
                        if not c.empty:
                            p = c.iloc[-1]; prev = c.iloc[-2]
                            chg = ((p-prev)/prev)*100
                            name = INDICES_CODES.get(t, t)
                            sector = "å¸‚å ´æŒ‡æ•¸" if "^" in t else "å¤§å®—å•†å“"
                            
                            idx_rows.append({
                                "Code": t, "Name": name, "Close": round(p, 2), 
                                "Daily_Chg%": round(chg, 2), "Daily_Amount_B": 0, 
                                "Volume": 0, "Sector": sector
                            })
                    except: continue
                
                if idx_rows:
                    df_official = pd.concat([df_official, pd.DataFrame(idx_rows)], ignore_index=True)
            except Exception as e: print(f"   âš ï¸ yfinance è£œæŠ“å¤±æ•—: {e}")
            
            df_final = df_official

    # 3. è£œå…… VIX
    if tw_vix:
        vix_row = {"Code": "^VIXTWN", "Name": "å°æŒ‡ VIX", "Sector": "æ³¢å‹•ç‡", "Close": tw_vix, "Daily_Chg%": 0, "Daily_Amount_B": 0, "Volume": 0}
        df_final = pd.concat([df_final, pd.DataFrame([vix_row])], ignore_index=True)

    # 4. å­˜æª”
    if df_final.empty:
        # è¬ä¸€çœŸçš„å…¨æ›ï¼Œå»ºç«‹ç©ºè¡¨é˜²æ­¢ crash
        STD_COLUMNS = ['Code', 'Name', 'Close', 'Daily_Chg%', 'Daily_Amount_B', 'Volume', 'Sector', 'Industry']
        df_final = pd.DataFrame(columns=STD_COLUMNS)

    df_final['Industry'] = df_final.get('Sector', 'å…¶ä»–')
    df_final['Vol_Increase'] = False
    for c in ['RVOL', 'Weekly_Chg%', 'Monthly_Chg%']: df_final[c] = 0
    
    # æ¼²åœç¾åŒ–
    def beautify_limit_up(pct):
        try:
            p = float(pct)
            if p >= 9.90: return 10.00
            if p <= -9.90: return -10.00
            return p
        except: return 0.0
    
    if 'Daily_Chg%' in df_final.columns:
        df_final['Daily_Chg%'] = df_final['Daily_Chg%'].apply(beautify_limit_up)

    csv_path = os.path.join(TARGET_DIR, f"rank_all_{DATE_STR}.csv")
    df_final.to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    generate_ai_analysis_placeholder()
    print(f"âœ… æ•¸æ“šæ›´æ–°å®Œæˆ: {csv_path} (ç¸½ç­†æ•¸: {len(df_final)})")

if __name__ == "__main__":
    fetch_and_process_data()
