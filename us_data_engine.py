import yfinance as yf
import pandas as pd
import numpy as np
import requests
import json
import os
import datetime
import time
import re
from io import StringIO

# ==========================================
# 0. è¨­å®šèˆ‡ç›®éŒ„æº–å‚™
# ==========================================
NOW = datetime.datetime.now()
YYYY = NOW.strftime("%Y")
MM = NOW.strftime("%m")
DATE_STR = NOW.strftime("%Y%m%d")

BASE_DIR = "us_stock_dashboard"
TARGET_DIR = os.path.join(BASE_DIR, YYYY, MM)

os.makedirs(TARGET_DIR, exist_ok=True)

print(f"ğŸ“‚ ç›®æ¨™è³‡æ–™å¤¾: {TARGET_DIR}")
print(f"ğŸ“… è™•ç†æ—¥æœŸ: {DATE_STR}")

# ==========================================
# 1. éœæ…‹è§€å¯Ÿåå–®
# ==========================================
STATIC_TICKERS = {
    "^DJI": {"Name": "Dow Jones", "Theme": "é“ç“Š"},
    "^GSPC": {"Name": "S&P 500", "Theme": "æ¨™æ™®"},
    "^IXIC": {"Name": "Nasdaq", "Theme": "é‚£æŒ‡"},
    "^SOX": {"Name": "PHLX Semi", "Theme": "è²»åŠ"},
    "^VIX": {"Name": "VIX", "Theme": "ææ…Œ"},
    "BTC-USD": {"Name": "Bitcoin", "Theme": "åŠ å¯†å¹£"},
    "NVDA": {"Name": "NVIDIA", "Theme": "AI"},
    "MSFT": {"Name": "Microsoft", "Theme": "è»Ÿé«”"},
    "AAPL": {"Name": "Apple", "Theme": "æ¶ˆè²»é›»"},
    "AMZN": {"Name": "Amazon", "Theme": "é›»å•†"},
    "GOOG": {"Name": "Alphabet", "Theme": "æœå°‹"},
    "META": {"Name": "Meta", "Theme": "ç¤¾ç¾¤"},
    "TSLA": {"Name": "Tesla", "Theme": "é›»å‹•è»Š"},
    "TSM": {"Name": "TSMC", "Theme": "æ™¶åœ“"},
    "AMD": {"Name": "AMD", "Theme": "æ™¶ç‰‡"},
    "AVGO": {"Name": "Broadcom", "Theme": "ç¶²é€š"},
    "SMCI": {"Name": "Super Micro", "Theme": "ä¼ºæœå™¨"},
    "COIN": {"Name": "Coinbase", "Theme": "å¹£æ‰€"},
}

ALL_TICKER_INFO = STATIC_TICKERS.copy()

# ==========================================
# 2. CNN ææ‡¼è²ªå©ªæŒ‡æ•¸ (é›™é‡æŠ“å–æ©Ÿåˆ¶)
# ==========================================
def fetch_fear_and_greed():
    print("æ­£åœ¨æŠ“å– CNN Fear & Greed æŒ‡æ•¸...")
    
    # å½è£æˆå®Œæ•´ç€è¦½å™¨
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://edition.cnn.com/markets/fear-and-greed",
        "Origin": "https://edition.cnn.com",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9"
    }

    result = None

    # --- æ–¹æ³• A: å˜—è©¦å®˜æ–¹ API ---
    try:
        url_api = "https://production.dataviz.cnn.io/index/fearandgreed/graphdata"
        resp = requests.get(url_api, headers=headers, timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            latest = data['fear_and_greed']
            score = int(latest['score'])
            rating = latest['rating']
            # æ ¡æ­£ Rating å¤§å°å¯«
            rating = rating.capitalize() if rating else "Neutral"
            
            result = {"score": score, "rating": rating, "timestamp": latest['timestamp']}
            print(f"âœ… [API] æˆåŠŸæŠ“å– CNN æŒ‡æ•¸: {score} ({rating})")
    except Exception as e:
        print(f"âš ï¸ [API] æŠ“å–å¤±æ•—ï¼Œåˆ‡æ›è‡³ç¶²é çˆ¬èŸ²æ¨¡å¼... ({e})")

    # --- æ–¹æ³• B: å¦‚æœ API å¤±æ•—ï¼Œå˜—è©¦çˆ¬ç¶²é åŸå§‹ç¢¼ (Regex) ---
    if result is None:
        try:
            url_web = "https://edition.cnn.com/markets/fear-and-greed"
            resp = requests.get(url_web, headers=headers, timeout=10)
            if resp.status_code == 200:
                # åœ¨ HTML ä¸­å°‹æ‰¾ "score":45 é€™æ¨£çš„å­—ä¸²
                html = resp.text
                # å°‹æ‰¾é¡ä¼¼ "fear_and_greed":{"score":45.321,"rating":"fear" é€™æ¨£çš„çµæ§‹
                match_score = re.search(r'"score":([\d\.]+)', html)
                match_rating = re.search(r'"rating":"([a-zA-Z\s]+)"', html)
                
                if match_score:
                    score = int(float(match_score.group(1)))
                    rating = match_rating.group(1).capitalize() if match_rating else "Neutral"
                    result = {"score": score, "rating": rating, "timestamp": datetime.datetime.now().isoformat()}
                    print(f"âœ… [Web] æˆåŠŸçˆ¬å– CNN æŒ‡æ•¸: {score} ({rating})")
                else:
                    print("âŒ [Web] æœªèƒ½åœ¨ç¶²é ä¸­æ‰¾åˆ°åˆ†æ•¸æ•¸æ“š")
        except Exception as e:
            print(f"âŒ [Web] çˆ¬èŸ²ä¹Ÿå¤±æ•—: {e}")

    # --- å­˜æª” ---
    filepath = os.path.join(TARGET_DIR, f"sentiment_{DATE_STR}.json")
    
    if result:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=4)
    else:
        # çœŸçš„å…¨å¤±æ•—ï¼Œå¯«å…¥éŒ¯èª¤æ¨™è¨˜ï¼Œä¸è¦å¯« 50ï¼Œä»¥å…èª¤å°
        print("âŒ CNN æŒ‡æ•¸å®Œå…¨ç²å–å¤±æ•—ï¼Œä½¿ç”¨ N/A æ¨™è¨˜")
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump({"score": 0, "rating": "Data Unavailable", "timestamp": ""}, f)

# ==========================================
# 3. å‹•æ…‹å¸‚å ´æƒæ (Web Scraping)
# ==========================================
def get_market_screeners():
    print("ğŸ” æ­£åœ¨çˆ¬å– Yahoo ç¶²é ç†±é–€æ¦œ (Web Scraping)...")
    
    targets = [
        ("https://finance.yahoo.com/most-active", "ğŸ”¥ äº¤æ˜“ç†±é–€"),
        ("https://finance.yahoo.com/gainers", "ğŸš€ æ¼²å¹…æ’è¡Œ")
    ]
    
    found_tickers = []
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    for url, tag in targets:
        try:
            print(f"   -> æ­£åœ¨è®€å–: {url} ...")
            r = requests.get(url, headers=headers, timeout=10)
            dfs = pd.read_html(StringIO(r.text))
            
            if len(dfs) > 0:
                df = dfs[0]
                symbols = df.iloc[:, 0].tolist()
                
                count = 0
                for sym in symbols:
                    sym = str(sym).split(" ")[0]
                    if "." not in sym and len(sym) < 6:
                        found_tickers.append(sym)
                        if sym not in ALL_TICKER_INFO:
                            try:
                                name = df.iloc[count, 1]
                            except:
                                name = sym
                            ALL_TICKER_INFO[sym] = {"Name": name, "Theme": tag}
                        count += 1
                print(f"      æŠ“åˆ° {count} æª”")
            else:
                print(f"      âš ï¸ æœªåœ¨é é¢ä¸­ç™¼ç¾è¡¨æ ¼")

        except Exception as e:
            print(f"      âš ï¸ çˆ¬å–å¤±æ•—: {e}")

    if not found_tickers:
        print("âš ï¸ çˆ¬èŸ²å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨ç†±é–€æ¸…å–®")
        backup_list = ["PLTR", "SOFI", "MARA", "RIOT", "DKNG", "UBER", "HOOD", "OPEN", "LCID", "RIVN", "AMD", "F", "BAC", "T", "INTC"]
        for sym in backup_list:
            if sym not in ALL_TICKER_INFO:
                ALL_TICKER_INFO[sym] = {"Name": sym, "Theme": "å‚™ç”¨ç†±é–€"}
        found_tickers = backup_list

    dynamic_list = list(set(found_tickers))
    print(f"âœ… æƒæå®Œæˆï¼å…±é–å®š {len(dynamic_list)} æª”æ´»èºè‚¡ç¥¨ã€‚")
    return dynamic_list

def fetch_and_process_data():
    fetch_fear_and_greed()
    
    dynamic_tickers = get_market_screeners()
    static_list = list(STATIC_TICKERS.keys())
    final_tickers = list(set(static_list + dynamic_tickers))
    
    print(f"ğŸš€ æº–å‚™ä¸‹è¼‰ {len(final_tickers)} æª”æ¨™çš„æ•¸æ“š...")
    
    results = []
    BATCH_SIZE = 8
    chunks = [final_tickers[i:i + BATCH_SIZE] for i in range(0, len(final_tickers), BATCH_SIZE)]
    
    for i, chunk in enumerate(chunks):
        max_retries = 5
        success = False
        data = pd.DataFrame()

        for attempt in range(max_retries):
            try:
                print(f"â³ ä¸‹è¼‰æ‰¹æ¬¡ {i+1}/{len(chunks)} (å˜—è©¦ {attempt+1}/{max_retries}): {chunk[:3]}...")
                data = yf.download(chunk, period="3mo", progress=False, auto_adjust=False, threads=False)
                if not data.empty:
                    success = True
                    break
            except Exception as e:
                print(f"   âš ï¸ éŒ¯èª¤: {e}")
            time.sleep(5)

        if not success or data.empty:
            print(f"âŒ æ‰¹æ¬¡ {i+1} æœ€çµ‚å¤±æ•—ï¼Œè·³éã€‚")
            continue
        
        for ticker in chunk:
            try:
                if len(chunk) == 1 or isinstance(data.columns, pd.Index) and not isinstance(data.columns, pd.MultiIndex):
                    closes = data['Close']
                    volumes = data['Volume']
                else:
                    try:
                        closes = data['Close'][ticker].dropna()
                        volumes = data['Volume'][ticker].dropna()
                    except KeyError: continue

                if closes.empty: continue
                
                current_price = closes.iloc[-1]
                # éæ¿¾æ°´é¤ƒè‚¡
                if current_price < 1.0: continue

                current_vol = 0 if volumes.empty else volumes.iloc[-1]
                
                if len(volumes) >= 6:
                    avg_vol_5d = volumes.iloc[-6:-1].mean()
                    rvol = (current_vol / avg_vol_5d) if avg_vol_5d > 0 else 0
                else:
                    rvol = 0

                def calc_chg(series, shift):
                    if len(series) > shift:
                        last = series.iloc[-1]
                        prev = series.iloc[-(shift + 1)]
                        return ((last - prev) / prev) * 100 if prev != 0 else 0
                    return 0

                def calc_vol_chg(series, shift):
                    if len(series) > shift:
                        last = series.iloc[-1]
                        prev = series.iloc[-(shift + 1)]
                        return ((last - prev) / prev) * 100 if prev != 0 else 0
                    return 0

                daily_chg = calc_chg(closes, 1)
                weekly_chg = calc_chg(closes, 5)
                monthly_chg = calc_chg(closes, 21)
                
                daily_vol_chg = calc_vol_chg(volumes, 1)
                weekly_vol_chg = calc_vol_chg(volumes, 5)
                monthly_vol_chg = calc_vol_chg(volumes, 21)
                
                amount_b = (current_price * current_vol) / 1_000_000_000
                info = ALL_TICKER_INFO.get(ticker, {"Name": ticker, "Theme": "Scan"})

                results.append({
                    "Code": ticker,
                    "Name": info.get('Name', ticker),
                    "Theme": info.get('Theme', 'Scan'),
                    "Close": round(current_price, 2),
                    "Volume": current_vol,
                    "RVOL": round(rvol, 2),
                    "Daily_Chg%": round(daily_chg, 2),
                    "Weekly_Chg%": round(weekly_chg, 2),
                    "Monthly_Chg%": round(monthly_chg, 2),
                    "Daily_Vol_Chg%": round(daily_vol_chg, 2),
                    "Weekly_Vol_Chg%": round(weekly_vol_chg, 2),
                    "Monthly_Vol_Chg%": round(monthly_vol_chg, 2),
                    "Daily_Amount_B": round(amount_b, 2),
                    "Weekly_Amount_B": round(amount_b, 2),
                    "Monthly_Amount_B": round(amount_b, 2)
                })

            except Exception: continue
        
        time.sleep(1)

    if not results:
        print("âŒ åš´é‡è­¦å‘Šï¼šç„¡æ•¸æ“šï¼")
    else:
        df_result = pd.DataFrame(results)
        for fname in [f"rank_daily_{DATE_STR}.csv", f"rank_weekly_{DATE_STR}.csv", f"rank_monthly_{DATE_STR}.csv"]:
            df_result.to_csv(os.path.join(TARGET_DIR, fname), index=False, encoding='utf-8-sig')
            
        print(f"\nâœ… æ•¸æ“šæ›´æ–°å®Œæˆï¼å…±è™•ç† {len(df_result)} æª”è‚¡ç¥¨ã€‚")

if __name__ == "__main__":
    fetch_and_process_data()
